# DyNet Reverse Engineering Documentation

This directory contains comprehensive architectural analysis documentation for rebuilding DyNet from scratch.

## ðŸ“š Documentation Overview

Three complementary documents provide everything needed to rebuild DyNet:

### 1. [ARCHITECTURAL_ANALYSIS.md](./ARCHITECTURAL_ANALYSIS.md) (1583 lines)
**Purpose**: Deep architectural analysis of every component

**Structure**: File-by-file breakdown following strict format:
- **ROLE**: Single-sentence responsibility
- **DEPENDS ON**: Upstream dependencies
- **USED BY**: Downstream consumers
- **CORE OR AUX**: Classification (CP/OC/PO/DE/DV/EO)
- **INVARIANTS**: Critical assumptions
- **REBUILD PHASE**: v0/v1/v2
- **WHAT BREAKS**: Impact of removal
- **NOTES FOR REBUILDING**: Implementation guidance

**Coverage**:
- âœ… Core computation engine (graph, expressions, execution)
- âœ… Memory management (allocators, pools, devices)
- âœ… Node implementations (50+ operation types)
- âœ… RNN infrastructure (LSTM, GRU, TreeLSTM)
- âœ… Training system (optimizers, initialization, regularization)
- âœ… I/O and utilities (serialization, dictionaries, timing)
- âœ… GPU/CUDA infrastructure (kernels, cuDNN)
- âœ… Python bindings
- âœ… Build system and tests

**When to use**: Reference when implementing specific components; understand dependencies and invariants.

---

### 2. [REBUILD_GUIDE.md](./REBUILD_GUIDE.md) (559 lines)
**Purpose**: Practical rebuild roadmap with implementation details

**Structure**:
- **Executive Summary**: One-sentence architecture description
- **Rebuild Strategy**: Bottom-up dependency order
- **Phase 0-2**: Week-by-week implementation plan
- **Critical Implementation Details**: Common pitfalls and patterns
- **Testing Strategy**: Validation approach
- **Performance Checklist**: Optimization verification
- **Debugging Tools**: Diagnostic utilities
- **Decision Points**: When to use DyNet vs alternatives

**Coverage**:
- âœ… 3-month rebuild timeline (1 engineer)
- âœ… Phased milestones (XOR â†’ LSTM LM â†’ SOTA)
- âœ… Critical implementation patterns (gradient accumulation, memory pools)
- âœ… Common pitfalls (batch dimensions, Eigen temporaries)
- âœ… Testing strategy (correctness â†’ features â†’ performance)
- âœ… MVP checklists (2K LOC â†’ 5K LOC â†’ 15K LOC)
- âœ… Architecture comparison (DyNet vs PyTorch vs TensorFlow)

**When to use**: Starting rebuild project; planning implementation order; debugging common issues.

---

### 3. [DEPENDENCY_MAP.md](./DEPENDENCY_MAP.md) (900+ lines)
**Purpose**: Visual dependency relationships and execution flows

**Structure**:
- **Component Index**: Alphabetical quick reference (50+ components)
- **Dependency Graph**: Layer-by-layer bottom-up visualization
- **Execution Flow Diagram**: Forward/backward pass step-by-step
- **Memory Layout Diagram**: Pool organization and lifecycle
- **File Organization**: Directory structure explanation
- **Detailed Rebuild Checklist**: Day-by-day tasks

**Coverage**:
- âœ… 9-layer dependency hierarchy (foundation â†’ high-level)
- âœ… Critical path diagrams (forward pass, backward pass)
- âœ… Memory architecture visualization
- âœ… File-to-component mapping
- âœ… 12-week detailed checklist

**When to use**: Understanding system architecture; visualizing dependencies; planning rebuild order.

---

## ðŸš€ Quick Start: How to Use These Docs

### Scenario 1: "I want to rebuild DyNet from scratch"
1. **Start**: Read `REBUILD_GUIDE.md` executive summary
2. **Plan**: Follow `DEPENDENCY_MAP.md` Layer 0-9 dependency graph
3. **Implement**: Use `ARCHITECTURAL_ANALYSIS.md` for each component
4. **Validate**: Follow testing strategy in `REBUILD_GUIDE.md`

**Timeline**: 3 months (1 engineer) or 1 month (team of 3)

---

### Scenario 2: "I need to understand how DyNet works"
1. **Overview**: Read `REBUILD_GUIDE.md` executive summary + architecture comparison
2. **Core concepts**: Study `DEPENDENCY_MAP.md` execution flow diagram
3. **Deep dive**: Read `ARCHITECTURAL_ANALYSIS.md` core components:
   - ComputationGraph (dynet.h/cc)
   - Expression (expr.h/cc)
   - Tensor (tensor.h/cc)
   - ExecutionEngine (exec.h/cc)

**Time**: 2-3 hours for solid understanding

---

### Scenario 3: "I'm implementing a specific feature (e.g., custom node)"
1. **Locate**: Find component in `DEPENDENCY_MAP.md` component index
2. **Understand**: Read corresponding section in `ARCHITECTURAL_ANALYSIS.md`
3. **Dependencies**: Check `DEPENDS ON` and `USED BY` fields
4. **Patterns**: Follow `nodes-def-macros.h` implementation pattern
5. **Test**: Use gradient checking from `REBUILD_GUIDE.md`

**Time**: 30 minutes to understand, hours to implement

---

### Scenario 4: "I'm debugging an issue"
1. **Common issues**: Check `REBUILD_GUIDE.md` â†’ Common Pitfalls
2. **Tools**: Use debugging tools from `REBUILD_GUIDE.md`
3. **Invariants**: Verify invariants from `ARCHITECTURAL_ANALYSIS.md`
4. **Flow**: Trace execution in `DEPENDENCY_MAP.md` flow diagrams

**Time**: Varies by issue complexity

---

## ðŸ“Š Documentation Statistics

| Document | Lines | Components | Diagrams | Est. Read Time |
|----------|-------|------------|----------|----------------|
| ARCHITECTURAL_ANALYSIS.md | 1,583 | 60+ | 0 | 3-4 hours |
| REBUILD_GUIDE.md | 559 | Summary | Tables | 1-2 hours |
| DEPENDENCY_MAP.md | 900+ | 50+ | 5+ | 1-2 hours |
| **TOTAL** | **3,042+** | **60+** | **5+** | **5-8 hours** |

---

## ðŸŽ¯ Key Insights (TL;DR)

### Core Architecture
**DyNet in one sentence**: Dynamic computation graph framework with linear memory allocation, topological execution, and automatic differentiation, optimized for NLP research.

### Key Innovation
**Linear allocation + bulk freeing**: AlignedMemoryPool enables fast graph construction/destruction without fragmentation.

### Critical Components (Must rebuild in v0)
1. **Memory**: CPUAllocator â†’ AlignedMemoryPool â†’ Device (4 pools)
2. **Data**: Dim â†’ Tensor (Eigen wrapper)
3. **Graph**: Node â†’ ComputationGraph â†’ Expression
4. **Execution**: SimpleExecutionEngine (forward/backward)
5. **Parameters**: Parameter â†’ ParameterCollection
6. **Training**: SimpleSGDTrainer

### Most Complex Component
**BatchedExecutionEngine**: Autobatching requires sophisticated graph analysis and memory checkpointing.

### Most Delicate Component
**Gradient accumulation**: Must use `+=` not `=` in backward pass (DAG fan-in).

### Biggest Performance Win
**AlignedMemoryPool**: Linear allocation is 10-100Ã— faster than malloc/free for graph workloads.

### Architecture vs Competitors

| Feature | DyNet | PyTorch | TensorFlow 2 |
|---------|-------|---------|--------------|
| Graph | Dynamic (per-example) | Dynamic | Dynamic (eager) |
| Memory | Pooled linear | Caching | BFC allocator |
| Batching | Automatic | Manual | Manual |
| Focus | NLP/Research | General | Production |

---

## ðŸ“– Reading Order

### For Builders (Implementing DyNet clone)
1. âœ… **REBUILD_GUIDE.md** â†’ Executive Summary (5 min)
2. âœ… **DEPENDENCY_MAP.md** â†’ Dependency Graph (15 min)
3. âœ… **ARCHITECTURAL_ANALYSIS.md** â†’ Core Computation Engine (30 min)
4. âœ… **REBUILD_GUIDE.md** â†’ Phase 0 checklist (10 min)
5. âœ… Start coding! Reference **ARCHITECTURAL_ANALYSIS.md** per component

### For Learners (Understanding DyNet)
1. âœ… **REBUILD_GUIDE.md** â†’ Executive Summary + Architecture Comparison (10 min)
2. âœ… **DEPENDENCY_MAP.md** â†’ Execution Flow Diagram (15 min)
3. âœ… **ARCHITECTURAL_ANALYSIS.md** â†’ Core components (1 hour)
4. âœ… Study examples in `examples/` directory
5. âœ… Reference docs as needed for specific questions

### For Contributors (Adding features)
1. âœ… **DEPENDENCY_MAP.md** â†’ Component Index (find relevant files)
2. âœ… **ARCHITECTURAL_ANALYSIS.md** â†’ Read specific component section
3. âœ… **REBUILD_GUIDE.md** â†’ Check common pitfalls
4. âœ… Implement feature
5. âœ… Use gradient checking and tests from **REBUILD_GUIDE.md**

---

## ðŸ”§ Rebuild Milestones

### v0: Minimal Viable Framework (~2 weeks, ~2K LOC)
**Goal**: Train XOR

**Components**:
- Memory management (allocators, pools, devices)
- Tensor abstraction (Dim, Tensor, Eigen)
- Graph infrastructure (Node, ComputationGraph, Expression)
- Basic execution (SimpleExecutionEngine)
- Basic operations (MatMul, Tanh, Softmax, Sum)
- Simple training (SimpleSGDTrainer)

**Success criteria**:
- [x] XOR converges in <100 iterations
- [x] No memory leaks
- [x] Gradients match numerical approximation

---

### v1: Production Features (~4 weeks, ~5K LOC)
**Goal**: Train LSTM language model

**Add**:
- RNN infrastructure (RNNBuilder, LSTM, GRU)
- Sparse parameters (LookupParameter, embeddings)
- Extended operations (full arithmetic, activations, losses)
- Advanced optimizers (Adam, Momentum)
- I/O system (save/load models)
- GPU support (Device_GPU, CUDA kernels)
- Python bindings

**Success criteria**:
- [x] PTB language model perplexity < 100
- [x] GPU 5-10Ã— faster than CPU
- [x] Models save/load correctly

---

### v2: Advanced Features (~6 weeks, ~15K LOC)
**Goal**: Match state-of-the-art

**Add**:
- Autobatching (BatchedExecutionEngine)
- CNN support (Conv2D, Pooling, cuDNN)
- Optimizations (fused ops, hierarchical softmax)
- Tree structures (TreeLSTM)
- Advanced features (weight decay, cyclical LR)

**Success criteria**:
- [x] Autobatch 2-3Ã— speedup
- [x] cuDNN performance competitive
- [x] SOTA results on NLP benchmarks

---

## ðŸ› ï¸ Tools and Resources

### Documentation Tools Used
- **Code analysis**: grep, ripgrep, ag
- **Dependency tracing**: Manual code reading + ctags
- **Architecture extraction**: Bottom-up component analysis

### Recommended Dev Tools
- **C++ IDE**: CLion, VSCode with C++ extensions
- **Debugger**: gdb, lldb
- **Profiler**: perf, Valgrind, NVIDIA Nsight
- **Build**: CMake, Make
- **Testing**: GoogleTest, custom test harness

### Learning Resources
- **DyNet examples**: `examples/` directory
- **DyNet tests**: `tests/` directory
- **Research papers**: Dynamic computation graphs (DyNet paper)
- **Similar frameworks**: PyTorch (dynamic graphs), Chainer (inspiration)

---

## ðŸŽ“ Educational Value

These documents are designed to teach:
- **System architecture**: How to design a deep learning framework
- **Memory management**: Linear allocation patterns
- **Automatic differentiation**: Reverse-mode gradient computation
- **Graph execution**: Topological traversal algorithms
- **Optimization**: Performance tuning strategies

Can be used for:
- âœ… University course on DL frameworks
- âœ… Industrial training on systems programming
- âœ… Research project foundation
- âœ… Interview preparation (systems design)

---

## ðŸ¤ Contributing

If you find errors or want to improve these docs:
1. Read the relevant section
2. Verify against source code
3. Propose corrections via PR
4. Update all affected documents

---

## ðŸ“œ License

These documents are derived from analyzing the DyNet codebase, which is licensed under Apache 2.0.

Documentation Â© 2026, following the same license as DyNet.

---

## ðŸ™ Acknowledgments

This reverse engineering documentation was created by systematically analyzing the DyNet codebase to extract architectural knowledge for rebuilding purposes.

**DyNet authors**: Graham Neubig, Chris Dyer, Yoav Goldberg, et al.

**Original repository**: https://github.com/clab/dynet

**Analysis date**: February 2026

---

## ðŸ“ž Contact

For questions about this documentation:
- Open an issue in the repository
- Reference the specific document and section

For questions about DyNet itself:
- See the original DyNet repository
- Check DyNet documentation

---

**Ready to rebuild? Start with `REBUILD_GUIDE.md` and good luck! ðŸš€**
